{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c28eb4",
   "metadata": {},
   "source": [
    "# PYNQ‑Z2 Lab Protocol: YUV Filter with Streams + DMA\n",
    "\n",
    "**Audience:** Embedded Systems students\n",
    "**Board:** PYNQ‑Z2 (Zynq‑7020)\n",
    "**Tools:** Vitis HLS, Vivado, PYNQ (Jupyter)\n",
    "**Goal:** Implement a streaming RGB→YUV→filter→RGB accelerator, integrate it with AXI DMA, and measure speedup vs. a Python baseline.\n",
    "\n",
    "---\n",
    "\n",
    "## Why filter in YUV?\n",
    "\n",
    "* **Perceptual separation:** YUV splits luminance (**Y**) from chrominance (**U, V**). Many visual tasks—brightness/contrast tweaks, denoising, edge enhancement—primarily affect **Y** without shifting color.\n",
    "* **Cleaner color handling:** Adjusting **Y** avoids RGB cross‑talk; color fidelity holds while brightness changes. You can also filter **U/V** (e.g., chroma noise reduction) independently.\n",
    "* **Bandwidth & compression awareness:** Video systems commonly operate in YUV (e.g., 4:2:0). Thinking in YUV maps to real pipelines students will meet in the wild.\n",
    "\n",
    "> In this lab we scale the **Y** channel, then convert back to RGB—simple, visible, and stream‑friendly.\n",
    "\n",
    "## Why `hls::stream`?\n",
    "\n",
    "* **Throughput via pipelining:** Streams let stages run concurrently (source → `rgb2yuv` → `scale_y` → `yuv2rgb` → sink).\n",
    "* **Backpressure built‑in:** AXI‑Stream handshakes (`TVALID/TREADY`) ensure no drops under bursty traffic.\n",
    "* **Small buffers:** Process pixels as they arrive; avoid full‑frame BRAMs.\n",
    "* **AXI‑Stream ready:** Streams map naturally to AXIS for DMA/video subsystems.\n",
    "\n",
    "## Why a DMA engine?\n",
    "\n",
    "* **Fast PS↔PL transfers:** AXI DMA moves large buffers between DDR and your accelerator without CPU copies.\n",
    "* **MM2S/S2MM:** Memory‑to‑Stream feeds pixels into the IP; Stream‑to‑Memory collects the results.\n",
    "* **Scales with image size:** Sustained throughput with minimal CPU involvement; clean timing measurements.\n",
    "\n",
    "## Part A — Software baseline (Python on PYNQ)\n",
    "\n",
    "1. Copy `yuv_filter_soft.ipynb` and a test image (e.g., `input.png`) to the board’s Jupyter folder.\n",
    "2. Run the notebook:\n",
    "3. Record the **time per frame**; this is your CPU baseline.\n",
    "\n",
    "---\n",
    "\n",
    "## Part B — Build three HLS IP cores in Vitis HLS\n",
    "\n",
    "Do these steps **three times** (one per top function): `rgb2yuv_ip`, `scale_y_ip`, `yuv2rgb_ip`.\n",
    "\n",
    "1. **Create HLS Component** (C/C++), clock **100 MHz**.\n",
    "2. **Add source:** the provided code; set the top function accordingly.\n",
    "3. **Interfaces:**\n",
    "\n",
    "   * `#pragma HLS INTERFACE axis port=stream_in`\n",
    "   * `#pragma HLS INTERFACE axis port=stream_out`\n",
    "   * For `scale_y_ip`: `#pragma HLS INTERFACE s_axilite port=scale_Y bundle=CTRL` and `s_axilite port=return bundle=CTRL`.\n",
    "4. **C Simulation:** Use a simple stream TB that pushes N pixels and sets `TLAST` on the last.\n",
    "5. **C Synthesis:** Aim for **II=1** per stage. Check utilization and timing @100 MHz.\n",
    "6. *(Optional)* C/RTL Co‑Sim to validate AXIS handshake and TLAST propagation.\n",
    "7. **Export RTL → Package IP**. Note the **repository folders** of all three IPs.\n",
    "\n",
    "---\n",
    "\n",
    "## Part C — Vivado Block Design: PS + DMA + 3× IP (AXIS chain)\n",
    "\n",
    "1. **New Project** → select **PYNQ‑Z2** board.\n",
    "2. **Create Block Design** (e.g., `system`).\n",
    "3. **Add `ZYNQ7 Processing System`** → **Run Block Automation** (DDR, FCLK0=100 MHz).\n",
    "4. **Add `AXI DMA`**:\n",
    "\n",
    "   * Enable **MM2S** and **S2MM**; disable Scatter‑Gather for simplicity.\n",
    "   * Connect DMA control (AXI‑Lite) to **PS M_AXI_GP0** (Connection Automation).\n",
    "5. **Add the three HLS IPs** (add their repo paths in *Settings → IP → Repository*): `rgb2yuv_ip`, `scale_y_ip`, `yuv2rgb_ip`.\n",
    "6. **Connect control of `scale_y_ip`** (AXI‑Lite) to PS M_AXI_GP0. Assign addresses in **Address Editor**.\n",
    "7. **AXI High‑Performance port:** Enable **S_AXI_HP0** on Zynq PS; connect DMA MM2S/S2MM **M_AXI** to **S_AXI_HP0** via SmartConnect.\n",
    "8. **AXI‑Stream chain:**\n",
    "\n",
    "   * **MM2S M_AXIS** → `rgb2yuv_ip` **s_axis**\n",
    "   * `rgb2yuv_ip` **m_axis** → `scale_y_ip` **s_axis**\n",
    "   * `scale_y_ip` **m_axis** → `yuv2rgb_ip` **s_axis**\n",
    "   * `yuv2rgb_ip` **m_axis** → **S2MM S_AXIS**\n",
    "9. **Data width and TKEEP:**\n",
    "\n",
    "   * If using **32‑bit** streams (as in the code), keep all AXIS links at 32 bits and set transfer length to **4×pixels**. TKEEP may be 0xF (all bytes valid; last byte unused).\n",
    "   * Alternatively, re‑synthesize all cores as **24‑bit** streams and insert **AXIS Data Width Converters** at DMA if needed.\n",
    "10. **Clocks/Resets:** Drive DMA and all IPs from **FCLK_CLK0** and a **Processor System Reset**. Route resets to each AXIS block.\n",
    "11. **Validate Design** (no DRCs), **Generate HDL Wrapper**, **Run Synthesis → Implementation → Generate Bitstream**.\n",
    "12. Collect artifacts: `system_wrapper.bit` and the `system.hwh` (search for `*.hwh`). Rename to shared base, e.g., `yuv.bit`, `yuv.hwh`.\n",
    "\n",
    "---\n",
    "\n",
    "## Part D — Run on PYNQ with DMA\n",
    "\n",
    "1. Copy to the board: `yuv.bit`, `yuv.hwh`, `yuv_filter_hard.ipynb`, and a test image.\n",
    "2. Run the notebook:\n",
    "3. Compare to the CPU result (timing and a quick visual diff). For a number, compute MSE:\n",
    "\n",
    "```python\n",
    "from yuv_filter_numpy import yuv_filter_numpy\n",
    "cpu = yuv_filter_numpy(np.array(Image.open('input.png').convert('RGB')), 1.15)\n",
    "import numpy as np\n",
    "mse = np.mean((cpu.astype(np.int16) - out_rgb.astype(np.int16))**2)\n",
    "print('MSE vs CPU:', mse)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "* **Diagram/Screenshot** of the Vivado Block Design (PS, DMA, DataWidthConv if used, HLS IP).\n",
    "* **Timing table** (CPU vs. HW) for at least two `scale_Y` values and two image sizes.\n",
    "* **Result images** (CPU and HW) for one case.\n",
    "* **Notes**: stream width choice, TKEEP/TLAST handling, clocking, any issues + fixes.\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "* **Can’t see IP in Vivado:** Add the correct repo folder (contains `component.xml`) and *Refresh*.\n",
    "* **DMA errors / hangs:** Ensure both DMA control ports are mapped, clocks connected, and `recv.transfer()` is called **before** `send.transfer()`; wait on both.\n",
    "* **No `.hwh` found:** It’s generated alongside the block design; search the project tree for `*.hwh`.\n",
    "* **Color looks off:** Remember the RGB↔YUV math uses limited‑range offsets (16/128). Check for extra scaling or incorrect channel order.\n",
    "* **Throughput low:** Enable `#pragma HLS DATAFLOW` and ensure each stage reads/writes the stream once per pixel (II≈1). Keep PL at 100 MHz initially.\n",
    "\n",
    "---\n",
    "\n",
    "## Optional extensions\n",
    "\n",
    "* Filter **U/V** channels (e.g., chroma denoise) and compare artifacts.\n",
    "* Swap to **VDMA** and operate on video frames/lines with `TLAST` per line.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
